![CI](https://github.com/vishalkoriyalearning/rag-serve/actions/workflows/ci.yml/badge.svg)
# ğŸš€ RAG-Serve â€” Retrieval Augmented Generation API

**RAG-Serve** is a production-ready, containerized **RAG (Retrieval-Augmented Generation)** system built with **FastAPI**, **FAISS**, **Sentence-Transformers**, and **LLMs (OpenAI + Ollama fallback)**.

It ingests documents â†’ chunks them â†’ builds embeddings â†’ stores a FAISS vector index â†’ retrieves relevant context â†’ and generates answers using LLMs.

This project demonstrates:

* Backend/API engineering
* RAG pipeline architecture
* Hybrid LLM usage (Cloud + Local)
* FAISS vector search
* Docker & CI/CD
* Test-driven workflow

---

## ğŸ“¦ Features

* **Document Ingestion** (`/ingest`, `/index-doc`)
* **Text Extraction** from PDF & TXT
* **Chunking** with overlap
* **Embeddings** using `sentence-transformers`
* **FAISS Vector Store** for fast retrieval
* **Query API** for semantic search
* **Generate API** using:

  * **OpenAI (Primary)**
  * **Ollama Llama3.2:1b (Fallback)**
* **Dockerized Deployment** (API + Ollama)
* **GitHub Actions CI**

  * Linting (Ruff)
  * Unit tests (PyTest)
  * Docker build
* **Simple, clean architecture**

---

## ğŸ§± Tech Stack

* **FastAPI** â€” API framework
* **FAISS** â€” vector similarity search
* **Sentence-Transformers** â€” embedding generation
* **OpenAI GPT models** â€” generation (optional)
* **Ollama Llama3.2:1b** â€” local generation fallback
* **Docker + Docker Compose**
* **GitHub Actions (CI)**
* **PyTest + Ruff**

---

## ğŸ“‚ Project Structure

```
rag-serve/
â”œâ”€ app/
â”‚  â”œâ”€ api/
â”‚  â”œâ”€ core/         # chunker, embeddings, vectorstore, generator
â”‚  â”œâ”€ utils/        # pdf text extraction
â”‚  â””â”€ main.py       # FastAPI root
â”œâ”€ configs/         # model configs
â”œâ”€ storage/         # FAISS + metadata
â”œâ”€ tests/           # pytest unit tests
â”œâ”€ docker-compose.yml
â”œâ”€ Dockerfile
â””â”€ README.md
```

---

## ğŸš€ Getting Started (Local)

### 1. Create virtual environment

```bash
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Run FastAPI

```bash
uvicorn app.main:app --reload
```

Visit â†’ [http://localhost:8000/docs](http://localhost:8000/docs)

---

## ğŸ³ Run with Docker (Recommended)

### 1. Build + start services (API + Ollama)

```bash
docker-compose up --build
```

### 2. Pull Ollama model (inside container)

```bash
docker exec -it ollama ollama pull llama3.2:1b
```

---

## ğŸ§ª Testing

Run all unit tests:

```bash
pytest -q
```

Lint the code:

```bash
ruff check .
```

---

## ğŸ”Œ API Endpoints

### âœ” Health Check

`GET /health`

### âœ” Ingest Document

`POST /ingest`

### âœ” Build Index

`POST /index-doc`

### âœ” Semantic Search

`POST /query`

### âœ” RAG + LLM Answer

`POST /generate`
Uses **OpenAI â†’ fallback to Ollama**

---

## ğŸ”„ CI/CD (GitHub Actions)

* Automatic linting
* Automatic tests
* Automatic Docker build
* Status badge included in this repo

---

## ğŸ“˜ Notes

This project is built as a **learning-by-doing** portfolio system to demonstrate knowledge of:

* AI engineering
* Modern backend design
* Vector search
* LLM integrations
* Dockerized microservices
* CI/CD automation

It is structured to be easily extended with:

* Reranking
* Chat memory
* UI client
* API authentication
* MLflow experiment tracking

